---
title: "Calculating the Proportionality Coefficient of Compositional Data with propr"
author: "Thomas Quinn"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

The bioinformatic evaluation of gene co-expression often begins with correlation based analyses. However, as demonstrated thoroughly in a recent publication, this approach lacks statistical validity when applied to relative data (Lovell 2015). This can include those biological data produced by microarray assays or high-throughput RNA-sequencing. As an alternative, Lovell et al suggest a *proportionality* metric, $\phi$, derived from compositional data analysis. [define Compositional data analysis]. A subsequent publication expounded these efforts by elaborating on another *proportionality* metric, $\rho$, for compositional data (Erb 2016). This package introduces a programmatic framework for the calculation of differential gene co-expression using the compositional data methods presented in the referenced publications.

$$\phi(A_i, A_j) = \frac{var(A_i - A_j)}{var(A_i)}

$$\rho(A_i, A_j) = 1 - \frac{var(A_i - A_j)}{var(A_i) + var(A_j)}

In the above formulae, we let $A_i$ and $A_j$ each represent a log-ratio transformed feature vector (e.g., a vector of $d$ gene values measured across $n$ conditions). Log-ratio transformation provides a means by which to normalize data in a manner that respects the nature of relative data. In other words, log-ratio transformation yields the same result whether applied to absolute or relative data.

In this package, we use two log-ratio transformations, the *centered log-ratio transformation* (clr) and the *additive log-ratio transformation* (alr). Although these transformations differ in definition, we sometimes refer to them jointly using the acronym ***lr**.

$$\textup{clr(x)} = \left[\ln\frac{x_i}{g(\textup{x})};...;\ln\frac{x_D}{g(\textup{x})}\right]

where $$g(\textup{x}) = \sqrt[D]{x_i...x_D}

$$\textup{alr(x)} = \left[\ln\frac{x_i}{x_D};...;\ln\frac{x_{D-1}}{x_D}\right]

The first difference between $\phi$ and $\rho$ is scale. The values of $\phi$ range from $[0, \infty)$, with lower $\phi$ values indicating more proportionality. The values of $\rho$ range from $[-1, 1]$, with greater $|\rho|$ values indicating more proportionality and negative $\rho$ values indicating inverse proportionality. A second difference is that $\phi$ lacks symmetry. However, one can force symmetry by reflecting the lower left triangle of the matrix across the diagonal with the argument `symmetrize = TRUE`.

A third difference is that []

## Calculating proportionality

We provide two principal functions for calculating proportionality. The first function, `phit`, implements the calculation of $\phi$ described in Lovell et al (2015). This function makes use of clr transformation exclusively. The second function, `perb`, implements the calculation of $\rho$ described initially in Lovell et al (2015) and expounded by Erb and Notredame (2016). This function makes use of either clr or alr transformation. For now, we will focus on the implementations using clr transformation, saving a discussion of alr transformation for later. Let us begin by building an arbitrary dataset of 4 features (e.g., genes) measured across 100 subjects.

```{r}
# Use data from Lovell phiDF.
set.seed(12345)



```


Let x represent any number of features measured across multiple biological replicates subjected to a binary or continuous event y. For example, y could represent case-control status, treatment status, treatment dose, or time. The `phit` and `perb` functions ultimately convert a "count matrix" with x rows and y columns into a proportionality matrix of x rows and x columns containing a $\phi$ or $\rho$ measurement for each feature pair. One can think of this matrix as equivalent to a distance matrix. Both functions return the proportionality matrix bundled within an object of the class `propr`. This object contains four slots:

* `@counts` A data.frame. Stores the original "counts matrix" input.
* `@logratio` A data.frame. Stores the log-ratio transformed "counts matrix".
* `@matrix` A matrix. Stores the proportionality matrix calculated by `phit` or `perb`.
* `@slot` pairs A data.frame. Projects the proportionality matrix pairwise.

```{r}
counts <- t(X)
phi <- phit(counts, symmetrize = FALSE)
rho <- 
```

## Subsetting propr objects

We have provided methods for conveniently subsetting objects belonging to the `propr` class. By using the familiar `$` and `[` methods, we can subset the *entire* `propr` object by any of the annotations found in the `@pairs` slot. Alternatively, using the `subset` function, we can subset the *entire* `propr` object by a vector of feature indices or names. In the examples below, we first subset by $\rho > .9$. Then, separately, we subset by the feature names "a" and "b".

```{r}

```

```{r}

```

## Empiric distributions

The proportionality metrics generated by the `phit` and `perb` functions do not permit an easy interpretation of "How proportional is proportional enough?". For this reason, we include an option to generate a null distribution of proportionality metrics through randomization of feature (e.g., gene) vectors. Effectively, this yields a kernel density function that estimates how likely `propr` would measure that degree of proportionality by chance alone. In its implementation, these empiric distributions consider only the absolute value of the proportionality metrics calculated by `perb`.

When using `phit` or `perb`, by setting the argument `iter` to a non-zero value, the `@pairs` slot of the resultant `propr` object will list an empiric p-value and FDR corrected p-value for each pair. The argument `iterSize` will specify how many features to include in each iteration. We emphasize here that these empiric distributions exist as a convenient guide for selecting features from ranked space and **not** as a means by which to form the basis of statistical hypothesis testing.

```{r}
set.seed(12345)

randomNum <- sample(1:1000, size = 2000 * 22, replace = TRUE)
counts <- matrix(randomNum, nrow = 2000, ncol = 22)
prop <- phit(counts, symmetrize = FALSE, iter = 50, iterSize = 500)

sum(prop$pval < .01) # 20,422 pairs show rare proportionality
sum(prop$fdr < .01) # ...but none survive FDR correction
```

## Visualizing pairs

Each feature (e.g. gene) belonging to a highly proportional data pair should show linearly correlated log-ratio transformed expression with one another across all subjects. The method `plot` provides a means by which to visually inspect whether this holds true. This function will plot **all** pairs in the `@pairs` slot. Therefore, we strongly recommend the user first subset the `propr` object before plotting. "Noisy" correlation between some feature pairs could suggest that the proportionality cutoff is too lenient.

```{r, fig.show = "hold"}
plot(prop[prop$fdr < .05, ])
```

## Additive log-ratio transformations

We recognize that this package utilizes some intuitively foreign concepts. Since log-ratio transformations of relative data comprise a major portion of proportionality analysis, we decided to dedicate some extra space to this topic. Let us begin by making some count data for 5 features (e.g., genes) labelled "a", "b", "c", "d", and "e", as measured across 100 subjects.

```{r}
N <- 100
a <- seq(from = 5, to = 15, length.out = N)
b <- a * rnorm(N, mean = 1, sd = 0.1)
c <- rnorm(N, mean = 10)
d <- rnorm(N, mean = 10)
e <- rep(10, N)
X <- data.frame(a, b, c, d, e)
```

First, let us assume that these data X represent absolute abundance counts. In the next Figures, we will compare absolute count scatterplots with relative count scatterplots. We see quickly how the comparison of relative data yields *spurious correlation*. Although genes "c" and "d" do not correlate with one another absolutely, their relative quantities do.

```{r, fig.show = "hold"}
pairs(X)
pairs(X / rowSums(X))
```

Likewise, if we did calculate correlation directly, we would discover that correlation coefficients differ for absolute and relative data. Here, we see numerically how relative data can yield spurious correlation.

```{r}
cor(X)
cor(X / rowSums(X))
```

By calculating the **variance of the log-ratios** (vlr), we arrive at a single measure of covariation that does not change with respect to the use of absolute or relative data.

```{r}
propr:::proprVLR(X)
propr:::proprVLR(X / rowSums(X))
```

In the same way, transforming a counts matrix using the **centered log-ratio** (clr) makes the data *subcompositionally coherent* (i.e., equivalent for absolute and relative data types). When calculating proportionality, we use (by default) the variance about the clr-transformed data to normalize the variance of the log-ratios (vlr) by the variance of its individual constituents. In this way, the use of clr-transformed data shifts the vlr-matrix onto a "standardized" scale that compares across all feature pairs. However, it does not guard against the possibility of *spurious proportionality*. In the next Figures, we compare absolute count scatterplots with relative count scatterplots. While equivalent, we see a relationship between "c" and "d" that should not exist, and one that is ultimately reflected (at least partially) in the results of `phit` and `perb` alike.

```{r, fig.show = "hold"}
X.clr <- propr:::proprCLR(X)
pairs(X.clr)
```

```{r, fig.show = "hold"}
phi <- phit(t(X))
phi@matrix
p <- perb(t(X))
p@matrix
```

With the **additive log-ratio** (alr), instead of using the geometric mean of a composition as the denominator, we select the value of one its components to use as a *reference*. If we select this reference as a feature (i.e., gene) with an *a priori* fixed expression across subjects, we can actually "back-calculate" absolute data from relative data. When crafting these data, we included "e" as a fixed constant. Let us perform alr-transformation of the relative data with "e" as the reference. We will see in this Figure that the spurious correlation between "c" and "d" disappears. Again, this gets reflected in the results of `perb` when we select a reference.

```{r}
X.alr <- propr:::proprALR(X / rowSums(X), ivar = 5)
pairs(X.alr)
```

```{r}
p <- perb(t(X), ivar = 5)
p@matrix
```

Now, let us assume these same data actually measure relative counts. In other words, `X` is already relative and we do not know the real quantities which correspond to `X` absolutely. Well, if we knew that "a" represented a known fixed quantity, we could use alr-transformation to "back-calculate" the absolute abundances. We will see that "c", "d", and "e" actually do have proportional expression under these conditions. Although the measured quantity of "c", "d", and "e" do not change, the measured quantity of a known fixed feature did change. Whenever "a" increased while "c", "d", and "e" remained the same, the latter three features actually decreased. Since they all decreased together, they acted as a highly proportional module. Again, this gets reflected in the results of `perb`.

```{r}
X.alr <- propr:::proprALR(X / rowSums(X), ivar = 1)
pairs(X.alr)
```

```{r}
p <- perb(t(X), ivar = 1)
p@matrix
```

## Experimental design

[spike-in discussion here]
